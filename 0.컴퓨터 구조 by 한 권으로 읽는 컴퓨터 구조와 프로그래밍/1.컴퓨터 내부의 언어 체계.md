# 문서 설명

제 개인적으로 생각하건데, 어쩌면 개발 공부의 첫 스타트를 끊을만한 **진짜 베이스 지식**이라고 여겨지는 부분입니다. 
정말 순수 노베이스인 사람도(당장 저부터가 노베이스^^) 이러한 정보를 앎으로서 시작한다면 큰 도움이 되지 않을까 싶습니다.

## 개요

우리가 사용하는 실제 문자 언어를 생각해보자. 문자 언어는 기호를 나열한 것이다. 언어마다 기호 자체가 달라질 수도 있고, 읽는 순서 역시 달라질 수 있으며, 기호의 순서에 따라 의미가 달라지기도 한다.
모든 언어의 뜻은 이러한 체계를 바탕으로 만들어진 기호의 집합으로 **인코딩**된다. 하지만 이것만으로는 충분하지 않다. 더욱 중요한 것은 바로 **문맥**인데 모두가 같은 문맥을 공유하여 **같은 기호에 같은 뜻을 부여할 수 있어야 한다.**

  - **인코딩**은 정보를 정확하고 일관되게 표현하기 위해 서로 약속된 규칙을 따르는 과정이다. 이 과정 덕분에 우리가 사용하는 문자나 데이터가 컴퓨터에서 올바르게 처리되고, 다른 컴퓨터나 기기에서도 동일하게 인식될 수 있다. 마치 사람들이 같은 언어를 이해하기 위해 같은 문맥과 규칙을 공유해야 하는 것처럼, 컴퓨터도 인코딩을 통해 정보를 올바르게 소통할 수 있게 된다.


**현실 세계에서도 그렇듯, 언제나 문맥을 명확히 식별할 수 있는 것은 아니다. 이는 컴퓨터 언어에도 마찬가지인 문제이다.**
여기서 말하는 '컴퓨터 언어'는 단순히 프로그래밍 언어만을 의미하지 않는다. 컴퓨터 언어란 **컴퓨터 내부의 전반적인 체계를 이해하기 위한 최소 단위의 표현 방식**이라 생각하면 된다. 이는 **이진법을 바탕으로 한** 데이터 표현, 논리 연산, 명령어 집합 등을 포함하는 개념이다.

## 비트

문자 언어의 틀을 이루는 세 가지 구성요소에 대해 적자면 다음과 같다.

  - 기호가 들어갈 상자
  - 상자에 들어갈 기호
  - 상자의 순서

이러한 구성요소는 컴퓨터 언어에서도 마찬가지라고 볼 수 있다. 먼저 상자에 대해 생각해보면, 자연어(현실 세계에서 사용되는 언어)에선 **문자**라고 부르는 것을 컴퓨터에선 **비트**라고 부른다. 

**비트**는 2진법을 사용한다는 뜻의 바이너리(binary)와 숫자를 뜻하는 디지트(digit)가 합쳐진 말이다.(여담으로 디지트라는 말은 10진수를 표현 10가지 기호를 뜻하는 말이다.) 비트는 2진법을 사용하는데, 오직 두 가지 기호를 여러가지 방식으로 조합해 이어붙여서 복잡한 정보를 표현한다. 여기서 중요한건 **기호는 추상적인 개념**이라는 사실을 기억해야한다. 이는 숫자가 될 수 있고 참 거짓이 될 수도 있다.

## 논리 연산

비트의 사용법 중 하나는 예/아니오 질문에 대한 답을 참/거짓으로 표현하는 것이다. 하지만 **하나의 비트로 표현할 수 없는 상황**이 많이 발생한다. 이러한 경우 **여러 비트를 조합해 문제를 해결**해야 하는데, 이때 사용하는 것이 바로 **논리 연산**이다. 논리 연산은 여러 비트를 조합하여 새로운 값을 만들어내는 방식으로, 대표적인 연산으로는 **AND, OR, NOT** 등이 있다.

### 불리언 대수

불리언 대수는 1800년대 조지 불이 만들어 낸 연산규칙의 집합으로서, 비트에 대해 사용할 수 있는 대표적인 연산 중 하나이다. 결합 법칙, 교환 법칙, 분배 법칙을 그대로 적용 가능하다.
기본적으로 불리언 연산자는 **AND, NOT, OR** 세 가지고, 합성 연산으로서 **XOR** 이 존재한다.

- **AND**
  - 둘 이상의 비트에 작용하는 연산으로서, 2비트 연산인 경우, 두 가지 비트가 모두 참일 경우에 결과가 참이 된다. 즉 **각 비트가 모두 참일 때** 연산의 결과가 참이 된다.
- **NOT**
    - 논리적 반대를 의미하며 하나의 비트에도 작용 가능하다. 예를 들어 어떠한 비트 하나가 참인 경우에 NOT 연산을 하면 거짓이 된다.
- **OR**
    - 이 연산도 둘 이상의 비트에 작용하는 연산으로, **각 비트 중 하나라도 참이라면** 연산의 결과도 참이 된다.
- **XOR**
    - 배타적(Exclusive) OR 이라는 뜻으로 첫번째 비트와 두번째 비트가 **서로 다를 때만 참이 된다.**
 
#### 드모르간의 법칙 

드모르간의 법칙은 두 개의 논리 연산식을 변환하는 데 사용되는 규칙으로, **NOT, AND, OR 연산 간의 관계를 재정립할 수 있다.** 이 법칙을 이용하면 특정 논리식을 더욱 간단하게 표현하거나, 특정 상황에서 논리 연산을 쉽게 이해할 수 있다. 

드모르간의 법칙은 다음과 같은 두 가지 주요 형태로 표현된다:

**1. NOT (A AND B) = (NOT A) OR (NOT B)** 

**2. NOT (A OR B) = (NOT A) AND (NOT B)**

즉, AND 연산과 OR 연산을 반대로 바꾸면서 각 항에 NOT을 붙이는 방식이다. 이 말은 NOT을 충분히 사용하면 AND를 OR로, 혹은 역으로도 대신할 수 있다는 것이다.
컴퓨터에서 입력을 항상 원하는 형태로 얻을 수는 없기 때문에 이러한 성질은 매우 유용하다.
저 법칙을 응용하여 다음과 같은 예시를 적을 수도 있다. 왼쪽에는 **긍정적인 논리**(정논리), 오른쪽에는 **부정적인 논리**(부논리)를 적었다.

```
            A OR B                                       (NOT A AND NOT B)                                   NOT(NOT A AND NOT B)
----------------------------------          ---------------------------------------------                  ------------------------
춥다 | 비가 온다 | 코트를 입는다                  NOT 춥다  | NOT 비가 온다 | NOT 코트를 입는다                      코트를 입는다
F        F          F                           F            F                F                                       F
F        T          T                  =>       F            T                F                   NOT                 T
T        F          T                           T            F                F                   =>                  T
T        T          T                           T            T                T                                       T
```

즉 A OR B 는 NOT(NOT A AND NOT B)와 같다는 사실이 도출된다. 이러한 드모르간의 법칙 사용은 **연산을 최소로 사용하여 비용을 최소화할 수 있다.**
<br>

## 정수를 비트로 표현

십진수를 생각해보자. 십진수는 밑이 10인 수 체계다. 이는 곧 자릿수가 10의 거듭제곱 꼴로 나타내진다. 5018이라는 숫자가 있을 때 이를 다르게 표현해보면
```
5*10^3 + 0*10^2 + 1*10^1 + 8*10^0 = 5018
```
이렇게도 표현이 가능하다. 

비트를 사용할 때도 이와 비슷하게 표현이 가능하다. 밑이 2라는 점 때문에 **기호를 담을 상자, 자릿수**가 더 많이 필요하다는 차이점이 있을 뿐이다.
예를 들어 숫자 9를 생각해볼 때, 십진수에선 한 자릿수로만 해결이 가능하다. 이진수는 **0과 1**만 사용이 가능하고 밑을 2로 가지기 때문에 1보다 큰 수를 표현할 때 부터 자릿수가 늘어난다. 두번째 상자, 즉 두번째 자릿수에는 어떤 이름이 붙을까?
바로 **2의 자리**이다. 이런식으로 자릿수에 따라서 2를 거듭제곱하여 값의 자리를 표현한다. 즉 숫자 '9'를 이진수로 표현하면 다음과 같은 결과가 나온다.

```
1*2^3 + 0*2^2 + 0*2^1 + 1*2^0 = 9 이를 이진수로 표현하면 -> 1001라는 4비트 수가 된다.
```

비트는 기호를 담을 상자라고 말을 했고, 여기서는 이진수의 자릿수로 표현이 된다고 하였다. 이는 다시 말해서 **임의의 비트로 값의 개수와 값의 범위를 산출 가능하다는 것이다.** 
다음 예시는 여러 가지 비트 개수를 사용해 표현할 수 있는 수의 범위를 보여준다.

```
비트 개수 | 값의 개수 | 값의 범위
    4          16        0-15
    8          256       0-255
    12         4096      0-4095
    16         65536     0-65535
    등등... 
```

2진수에서 가장 오른쪽의 비트를 **가장 작은 유효 비트**(**least significant bit**) 라고 부르고, 가장 왼쪽의 비트를 **가장 큰 유효 비트**(**most significant bit**)라고 부른다.

- **이 부분에서 나오는 일부 개념들은 차후 자세히 다룰 예정.**
  
  - **가장 작은 유효 비트**(LSB)
    - LSB는 이진수 표현에서 가장 오른쪽에 위치한 비트를 의미한다. LSB는 숫자 값에 가장 작은 영향을 미치는 비트로, 이 비트를 바꾸면 전체 값의 변화폭이 가장 작다.
    - 2진수의 특성상 LSB가 1이면 홀수, LSB가 0이면 짝수가 된다. 이 방식으로 홀/짝수 확인을 빠르게 할 수 있다.
   
  - **가장 큰 유효 비트**(MSB)
      - MSB는 숫자 값에 가장 큰 영향을 미치며, 이 비트를 바꾸면 값의 변화폭이 가장 크다.
      - 후술하겠지만, 부호 있는 정수에서 MSB는 부호를 결정하는 역할도 한다.
   
  - **공통 사항**
    - 데이터 전송 시 MSB First와 LSB First 방식으로 구분할 수 있다.
    - 예를 들어, 네트워크 전송 시 프로토콜에 따라 먼저 전송할 비트를 결정하는 데 MSB나 LSB가 중요한 역할을 한다.
    - **빅 엔디언**(Big-Endian) 방식은 MSB를 먼저 전송하고, **리틀 엔디언**(Little-Endian) 방식은 LSB를 먼저 전송한다.
    - 이러한 방식은 **CPU 아키텍처**에 따라 달라지며, 데이터 전송의 호환성 문제를 다룰 때 **엔디언**을 고려해야 한다.
   
또한 MSB에 리딩제로라고 불리는 0을 추가하면 어떤 값을 표현하는 데 필요한 최소한의 자릿수보다 더 많은 자릿수를 추가할 수 있다. 이는 컴퓨터가 **미리 정해진 수의 비트**를 한 덩어리로 사용하도록 만들어졌기 때문에, 2진수를 쓸 때는 이런식으로 항상 일정한 개수의 비트를 사용해 값을 표현하는 경우가 종종 있다.(예시 : 10 이라는 이진수를 0010으로 표현 가능)
   
### 2진수의 덧셈

2진수의 덧셈은 기존에 10진수로 계산하던 방식과 비슷하다. LSB에서 MSB 쪽으로 더하여 각 자릿수가 1보다 크다면 1을 다음 자리로 올린다.
**중요한건 컴퓨터 내부에서 이 덧셈을 어떻게 연산하느냐인 것이다**

이 파트에선 자세히 다루지 않겠지만, 2진 덧셈을 위에 언급한 **불리언 연산**으로도 표현 가능하다. 두 비트를 서로 더한 결과는 두 비트를 **XOR** 한 값과 같고, 올림은 두 비트를 **AND** 한 값과 같다.

덧셈 결과가 우리가 사용하기로 정한 비트의 개수를 벗어나는 범위의 수라면 어떻게 될까? 이럴 경우에 그 유명한 **오버플로우**(Overflow)가 발생한다. 이는 곧 MSB에서 올림이 발생했다는 뜻이다.
예를 들어, `1001`과 `1000`을 더한 결과는 2진수로 표현하면 `10001`이다. 그런데 비트의 개수가 4로 한정되는 경우엔 MSB쪽에 사용할 수 있는 비트가 없어 **`0001`** 이라는 결과가 나오게 된다.
  - 이 파트에선 다루진 않겠지만, **조건코드 레지스터** 라는 것이 있는데 이런 정보 중에선 **오버플로 비트**라는 것이 있다.
  - 이 비트에서는 MSB에서 발생한 올림값이 들어가, 이 비트값을 보면 오버플로우가 발생했는지 알 수 있다.
  - MSB 위쪽에서 1을 빌려오는 경우를 **언더플로** 라고 부른다.

### 음수 표현

뺄셈은 어떠한 수에 다른 수의 음수를 더하는 것과 같다. 즉 덧셈을 넘어 뺄셈을 표현하고자 하려면 음수 표현이 중요하다.

우리는 흔히 음수와 양수를 구분할 때 부호를 사용한다. **컴퓨터에선 MSB를 부호에 사용하기로 약속했다.** 즉 4비트를 사용할 때 MSB를 부호 기호로 사용하면 남는 경우의 수는 0-7까지 표현이 된다.
부호 비트가 **0이면 양수** **1이면 음수**라고 취급하기로 했다.

한 비트를 부호에 사용하고 나머지 비트를 수의 크기, 즉 0부터의 거리를 표현하기 위해 사용하는 방법을 **부호화 크기** 표현법이라고 한다. 다만 이러한 표현법은 아래와 같은 이유로 널리 쓰이지 못하고 있다.

1.비트를 구상하려면 비용이 드는데 **0을 표현하는 방법이 두가지로 생겨버린다.**

2.위에 잠깐 언급했던 XOR과 AND를 통한 덧셈을 사용할 수가 없다.
  - 예를 들어 1과 -1을 더하고 싶다고 칠 때 실제 결과값은 0이지만, 부호화 크기 표현법으로 이를 계산하면 **-2가 되어버린다**
```
0001 + 1001 = 1002
```

그럼 두가지 문제점을 해결하는 방법은 무엇일까? 바로 **2의 보수** 표현법을 사용하는 것이다.

예를 들어 4비트짜리 공간에 +1을 표현하여, 0001이라는 수를 써보자. 이 비트를 전부 뒤집은 1110이라는 수에(여기서 멈추면 1의 보수 표현법이다) +1을 더하는 것이다. 즉 **-1을 4비트 기준 1111로 표현하는 것이다**
이렇게 되면 0이라는 숫자는 4비트 기준, 0000이라는 하나의 수로만 표현되고, 기존의 덧셈을 그대로 사용할 수 있다. 여기서 중요한건 **사용하기로 정한 비트의 개수가 명확할 때 오버플로우를 이용하는 것이다.**

예를 들어 우리가 총 4비트를 이용한다고 하고, 0001과 1111을 더한다고 치자. 계산을 하면 실제로는 10000이라는 이진수 결과값이 나오지만 최대 MSB를 벗어나게 되어 우리가 실제로 얻어내는 값은 **0000**이 된다.
<br>

## 실수 표현

밑이 10인 실수에는 우리가 흔히 쓰는 10진 소수점이 포함된다. (0.457, 1.5, 900.08 등등...) 마찬가지로 밑이 2인 경우, 실수를 표기하기 위해 2진 소수점을 표현할 방법이 필요하다.

### 고정소수점 표현법
2진수를 사용해 소수를 표현하기 위해 2진 소수점의 위치를 임의로 정하는 방법이 있다. 예를 들어 4비트가 있다고 치면, 왼쪽 2비트는 정수를 표현하는데 쓰고 오른쪽 2비트는 분수들을 표현하는데 사용하는 것이다.
소수점의 위치가 항상 일정하기 때문에 이런 방식을 **고정소수점** 표현법이라고 부른다. 아래에는 간단한 예시를 적어놨다.

```
정수부분  |  분수부분  |    값
  0|0    .     0|1      1/4(0.25)
  0|1    .     1|0      1+1/2(1.5)
```

즉 분수 부분의 자릿수에 따라 **2의 거듭제곱을 분모로 사용하는 것이다** 이러한 접근 방식은 작동 자체에는 문제가 없다. 문제는 **쓸모있는 범위의 실수값을 표현하기 위해 필요한 비트 개수가 너무 많다.**
우리가 흔히 사용하는 컴퓨터에는 잘 사용하지 않는 방식이다.(다만 디지털 신호 처리 장치 등 특별한 목적에 쓰이는 일부 컴퓨터에는 사용하기도 함)

### 부동소수점 표현법

위에 있는 고정소수점을 이용하여 실수를 표현할 때, `플랑크 상수`부터 `아보가드로 수`에 이르는 범위의 값을 표현하려면 무려 **수백 비트**를 사용해야한다.
  - 플랑크 상수의 값은 6.63 * 10^-34 (J/s), 아보가드로 수는 6.02*10^23/몰이라는 수이며, 두 수의 범위는 2^191 정도 된다. 이를 표현하려면 거의 200비트가 필요하다...
이를 해결하기 위해 `과학적 표기법`을 2진수에 적용시킨다. 예를 들어서, 10진 소수점 왼쪽이 한 자리 뿐인 소수(가수라고 부른다)에 10의 거듭제곱한 값을 곱하는 방식으로 표기한다.

```
0.012 = 1.2 * 10^-2
```

이를 밑이 2인 2진수에도 적용이 가능해진다. 여기서 유의해야할 점은 **지수의 밑인 2라는 숫자는 비트로 표현할 필요는 없다는 것이다.** 정의상 밑 2는 항상 정해져 있다. 아래는 간단한 예시들이다.

```
가수  |  지수  |   값
0  . 0   0|0    0(0*2^0)
0  . 1   0|0    0.5(1/2 * 2^0)
0  . 1   0|1    1.0(1/2 * 2^1)
1  . 0   1|1    8.0(1 * 2^3)
```

이러한 표현법은 비효율성이 상당하다. 예시에는 안나왔지만, 4비트 기준으로 0을 표현하는 방법이 4가지나 될 뿐더러 다른 숫자도 두 가지 씩 있는 경우가 허다하다. 또한 비트 패턴이 가능한 모든 수를 표현 불가능하다. 
**예를 들어 6.5를 표현하는 비트 패턴이 마땅히 없기 때문에 6.0에 해당하는 수와 0.5에 해당하는 수가 있더라도 둘을 더할 수는 없다.** 이를 보완하고자 **IEEE 부동소수점 수 시스템** 이라는 표준 방법을 이용하고 있다.

### IEEE 부동소수점 수 표준

이 방법은 위에서 쓴 4비트 부동소수점 표현보다 훨씬 많은 비트를 사용하며, 가수와 지수에 대해 각각 부호 비트를 사용한다.(다만 지수에 대한 부호 비트는 지수의 비트 패턴에 감춰져있다.)
IEEE 표준이 똑같은 비트를 사용하더라도 정밀도를 가능한 한 높이는 방법으로는 두 가지가 존재한다. 한 가지는 **정규화**이다

정규화는 가수를 조정해서 맨 앞에 0이 없게 만드는 것이다. 정확히는 **숫자에 1을 앞에 붙이는 것이 아니라 소수점 위치를 조정하여 가수를 `1.xxxx` 형태로 만드는 것**이다.

```
0.511을 이진수로 표현하면 약 0.100000101110000101... 이라는 수가 나오는데 이를 1.0000010111... × 2^-1 처럼 표현하는 것이다.

이런 식으로 가수를 조정하면 당연히 지수도 조정해야한다.
```
두 번째 방법은 어차피 가수의 맨 왼쪽 비트가 1이라는 사실을 우리는 모두 인지하고 있으므로, **이를 생략하여 가수에 1비트를 더 사용하는 것이다.**


우리는 이 표준 방법에서 두 가지 부동소수점 수가 자주 쓰인다는 사실을 알아둬야한다. 하나는 **기본정밀도 부동소수점 수**(단정도 실수)이고, 또 하나는 **2배 정밀도 부동소수점 수**(배정도 실수)이다.
기본 정밀도 수는 32비트를 사용하며 7비트 정밀도로 대략 `+-10^+-38` 정도의 범위를 표현 가능하고 2배 정밀도 수는 64비트를 사용하기 때문에 더 넓은 범위를 표현 가능하다. 2배 정밀도는 15비트 정밀도로 대략 `+-10^+-308` 범위의 수를 표현할 수 있다.


**단정도 실수**
![d20c054d-39fb-4ca7-8a8b-2ad7a644ca86](https://github.com/user-attachments/assets/dbd643d1-c717-44ce-84df-8356edfb7f9a)

**배정도 실수**
![fc531870-d8ea-4da6-9d62-8eafc7f7af1b](https://github.com/user-attachments/assets/2e98a82a-1973-42e5-87a9-3ae15c9d90c1)


두 형태 모두 가수에 대한 부호를 사용한다. 배정도 실수가 단정도 실수에 비해 지수가 3비트 더 큰데, 이는 곧 지수의 범위가 8배 더 크다는 것이다. 또한 배정도 실수는 단정도 실수에 비해 가수가 29비트나 더 크다. 정밀도 역시 큰 차이를 보인다.
**다만 이 모든 장점은 비트를 2배 더 많이 사용함으로서 얻은 것이다.**

지수에 대해 부호 비트가 따로 존재하지 않는데, IEEE 754 설계자들은 지수 비트가 모두 0이거나 1인 경우에 특별한 의미를 갖게 하고 실제 지숫값은 나머지 비트 패턴에 넣고 싶었다. 이를 **편향된** 지숫값을 사용해 달성하였다. 

기본 정밀도의 경우 편향값은 127로 이진수 01111111이 지수 0을 표현한다. 00000001은 -126을 표현하는 식이며 1111110은 +127을 표현한다. 2배 정밀도의 경우는 127 대신 1023을 사용한다.


#### 여기서 얻을 수 있었던 생각..

실제 실수를 사용할 때 우리는 진짜 그 값 그대로 사용하는게 아니라 **근사치**를 사용하는 것을 깨달았다. 그 이전으로 넘어가 컴퓨터 이론에는 생각보다 **그렇기로 약속하자** 라는 식의 것들이 상당수였다. 또한 이런 정보들이 실제 내가 사용하는 프로그래밍 언어와 맞물리는 순간에는 정말 감탄이 나올 지경이었다. 예를 들어 위에서 설명하는 단정도/배정도 실수를 실 구현한게 자료형 **float과 double** 아닌가.


## 2진수를 다루는 쉬운 방법

2진수는 솔직히 가독성이 매우 구리다. 대충 1000이 넘어가는 숫자를 표현하면 비트가 최소한 10개는 족히 필요하다. 나같은 개발 새싹도 이런 생각을 하는데 당시 사람들은 오죽할까... 사람들은 2진수를 더 읽기 쉽게 표현할 수 있는 방법을 고안해냈다.

### 8진 표현법

예를 들어 100101110001010100과 같이 매우 긴 2진수가 있다고 치자. 이를 8진수로 바꾸는 것이다. 3비트를 0부터 7까지의 값으로 표현할 수 있다는 사실에 근거한 표현법이다.

```
100|101|110|001|010|100    | 2진수를 3비트 단위로 자르고
 4 | 5 | 6 | 1 | 2 | 4     | 8진 숫자로 바꾼다.(어차피 3비트 내에선 8진수의 모든 수가 표현 가능하므로, 해당하는 3자리 2진수를 우리가 아는 10진수의 값들로 바꿔도 무방하다)
```

### 16진 표현법

8진 표현법이 아직 쓰이기는 하나, 요즘에는 **16진 표현법** 이 쓰이고 있다. 이유는 요즘엔 컴퓨터 내부가 8비트의 배수를 사용해 만들어지기 때문이다. 8의 배수는 `4(16진수 한 자리의 비트)`로는 균일하게 나눠지지만, `3(8진수 한 자리의 비트 수)`으로는 나눠지지 않는다.
**16진수를 표현할 때의 문제는 우리가 가지고 있는 숫자는 10개라는 사실이다.** 우리에겐 10부터 15까지 표현할 6개의 숫자가 더 필요하다. 하여 **abcdef(혹은 ABCDEF)라는 기호가 10부터 15까지의 숫자를 표현한다고 믿기로 했다.**


### 진법 표기법

우리가 진법을 표기할 때는 숫자 아래에 작은 첨자를 붙여 해당 진법을 일관되게 나타낼 수 있었다. 
그러나 컴퓨터에서는 아래 첨자를 쉽게 입력하기 어렵기 때문에, 이를 대신할 일관성 있는 다른 표기법을 생각해야 했다. 
많은 사람들이 더 나은 방법이 있다고 주장하며 새로운 표기법을 개발하면서, 결과적으로 여러 가지 표기법이 생겨났다. 다음은 대표적으로 사용되는 표기법들이다.

1. 0으로 시작하는 숫자는 8진 숫자다. 017은 8진수이며 값은 10진수로 15다.
2. 1부터 9사이의 숫자로 시작하는 숫자는 10진수다.
3. 0x가 앞에 붙은 숫자는 16진수다. 예를 들어 0x12f는 16진수이며 값는 10진수303이다.
4. 0은 어차피 어떠한 진법에서나 0을 가리키기 때문에 상관이 없다.

2진수를 문맥만 보고 알아내기는 어렵고, 2진수를 직접적으로 사용할 일도 별로 없어서 2진수 표현을 제공하는 프로그래밍 언어는 거의 없다.(그나마 C++ 같은 언어는 0b 라는 접두사를 사용한다.)


## 비트 그룹의 이름

비트는 너무 작아서 기본 단위로 사용하기에는 유용성이 떨어진다. 따라서 비트를 좀 더 큰 덩어리로 조직화해야한다. 초기에는 수많은 아이디어가 있었지만, 세계적으로 **8비트 덩어리**가 널리 쓰이기 시작했고, 이를 **바이트byte**라고 부른다.

**워드**라는 묶음도 존재하는데, 이는 각 컴퓨터가 설계상 자연스럽게 사용할 수 있는 비트 묶음의 크기를 가리키는 말로 쓰인다.
  - 우리가 흔히 들어본 32비트 컴퓨터, 64비트 컴퓨터라는 말이 있을거다. 혹은 윈도우 32비트/64비트 라는 말 역시 많이 들어봤을 것.
  - 저 컴퓨터가 설계상 자연스럽게 사용할 수 있는 비트 묶음이란 것은 **CPU가 한 번에 다룰 수 있는 데이터의 양**을 의미한다. 이는 메모리에서 데이터를 가져오거나, 레지스터에서 연산할 때 32비트 길이의 데이터를 다룬다는 의미이다.
  - CPU가 한 번에 다룰 수 있는 데이터 양은 CPU의 클럭 속도(GHz 단위)로 결정되며, 이를 통해 명령을 실행하거나 데이터를 처리하는데 걸리는 시간을 결정하게 된다.(만약 CPU의 클럭 속도가 3GHz라면, 이는 CPU가 **초당 30억 번**의 클럭 사이클을 수행한다)
  - **요약하자면 워드는 CPU가 한 번의 클럭사이클 동안 가져올 수 있는 데이터의 양을 말하는 것이다.**

또한 큰 수를 다루기 위한 표준 용어들도 존재하는데 우리에게 친숙한 킬로바이트, 메가바이트 등이 바로 그러하다. 원래는 미터법에서 사용하던 메가, 기가 등등 이런 용어를 빌려와서 밑이 2인 값들을 표현한 것이다.
실제로 밑이 10이 아니라 2이기 때문에 그나마 가장 근접한 값들(1000은 1024 처럼)을 이용해 표현했다.
<br>

## 텍스트 표현

이번에는 수를 사용해 문자나 키보드에 있는 다른 기호 등을 표현하는 방법을 알아보자.

### 아스키 코드

**정보 교환을 위한 미국 표준 코드**(**ASCII, American Standard Code for Information Interchange**)의 줄임말인 아스키 코드가 비트를 이용해 텍스트를 표현하는 대표적인 방법 중 하나이다. 아스키는 모든 기호에 대해 7비트 수 값을 할당했다.
여기서 말하는 모든 기호는 위에 풀네임을 보듯이 **1963년 당시 미국 기준**이다.
아스키 코드에는 글자를 출력하는 데 쓰이는게 아닌 장치 제어를 위해 쓰이는 코드들도 있는데 이를 **제어 문자**라고 부른다.

사실 아스키 코드 표 자체는 인터넷에 매우 잘 나와있기 때문에 표 자체를 올리지는 않겠다..

### 다른 표준의 진화

아스키는 말 그대로 **영어**를 표현하는 데 필요한 모든 문자를 포함하고 있어서 상당 기간동안 표준 역할을 했다. 이는 곧 컴퓨터가 전세계적으로 널리 쓰이게 됨에 따라 그 밖의 언어를 지원해야만 할 필요가 점점 생겼다는 의미이다..
초기에는 각 국가들은 자신들의 언어에 맞는 별도의 표준을 만들었다. 이유는 단순하다. **비트가 지금보다 더 비싼 시절에 표준이 만들어졌기 때문에 억지로 7-8비트에 욱여넣느라 그런 것이다.**

그러다 기술이 점점 발전 되어 비트 가격이 떨어짐에 따라 그 유명한 **유니코드** 라는 새로운 표준이 만들어졌다. 당시 유니코드에는 문자에 16비트 코드를 부여했다. 여기서 중요한건 **유니코드는 문자가 어떤 코드 포인트를 가지는지에 대한 약속이지, 이 문자를 어떻게 저장할지를 정의하는 것은 아니란 것이다.**


### 유니코드 변환 형식 8비트

컴퓨터는 7비트 값을 처리하도록 설계되지 않았기 때문에, 실제로는 8비트를 사용해 아스키 문자를 저장한다. 과거에 비해 비트의 비용이 많이 낮아졌지만, **여전히 8비트만 사용해도 충분히 많은 문자를 표현할 수 있는 상황에서 굳이 16비트를 사용하기에는 비용적인 측면에서 무리가 있었다.** 따라서 한 문자를 8비트로 표현하는 방식을 사용하게 되었다.

하지만 유니코드는 단순히 아스키 이상의 다양한 언어와 기호를 표현해야 하는 필요성에서 출발하였다. 이를 위해 **유니코드는 문자 코드에 따라 각기 다른 길이의 인코딩 방식**을 사용하는데, 이는 UTF-8이라는 변환 형식을 통해 이러한 문제를 효율적으로 해결한다.

#### 인코딩과 디코딩

문서 최상단에 언급된 인코딩이란 말을 다시 정의해보자. **인코딩**(**Encoding**)이란, 문자나 데이터 같은 정보를 특정한 형식으로 변환하여 컴퓨터가 이해하고 처리할 수 있도록 하는 과정을 말한다. 
컴퓨터는 근본적으로 0과 1로 이루어진 이진수로만 데이터를 처리할 수 있기 때문에, 우리가 사용하는 글자나 이미지, 소리 등의 정보도 결국엔 이진수로 변환되어야 한다. 이 과정을 인코딩이라고 한다.

그럼 디코딩은 무엇일까? 간단하다. **디코딩**(**Decoding**)은 그 반대 과정이다. 컴퓨터 내부의 이진수 데이터를 다시 사람이 읽을 수 있는 문자나 정보로 변환하는 것을 말한다.

#### 인코딩에서 발생할 수 있는 문제

- **문자 깨짐**: 인코딩 방식이 잘못 지정되거나 서로 다른 인코딩 방식을 사용하는 경우, 원래의 문자가 다른 의미의 문자나 이상한 기호로 보이는 문제가 발생할 수 있다. 이를 흔히 '문자 깨짐' 현상이라고 한다.

- **인코딩과 디코딩 불일치**: 데이터를 인코딩할 때 사용된 방식과 디코딩할 때 사용된 방식이 다르면 올바르게 해석되지 않게 된다. 이 때문에, 시스템 간에 데이터를 주고받을 때 항상 동일한 인코딩 방식을 사용하는 것이 중요하다.


그렇다면 유니코드 변환 형식 8비트(이하 **UTF-8**)은 무엇일까? UTF-8은 켄 톰슨과 롭 파이크가 만든 인코딩 방법으로 하위 호환성과 효율성 때문에 가장 널리 쓰이고 있는 인코딩 방법 중 하나이다.
구체적으로 말하자면, UTF-8은 기존 아스키 코드처럼 **8비트 내에 처리할 수 있는 경우에는 8비트를 그대로 사용**하고, 그 외 문자들에 대해서는 **여러 개의 8비트를 조합**하여 표현하는 가변 길이 인코딩 방식이다.

UTF-8는 MSB쪽 8비트 덩어리의 비트들이 길이를 표현하여 덩어리의 맨 앞을 식별하기 쉽게 만들어졌다. 예를 들어 아스키 코드는 하나의 8비트 덩어리(바이트)에서 해결이 가능하기때문에 맨 앞 MSB에는 무조건 0이 붙게된다. 밑에는 간단한 예시다.
  - 여담으로 영어의 경우 이러한 성질 덕에 타 언어권보다 더 적은 용량으로 문자 인코딩이 가능하여 영어 사용자에게 매우 큰 편리함을 준다..
```
유니코드상 A를 나타내는 이진수

0000000001000001(65, 0x0041)

ㄴ> 여기서 UTF-8은 MSB에 몇 바이트로 구분되는지 구분할 수 있는 식별 비트인 0을 넣고 실제 값만 가져와 다음과 같이 인코딩한다.

01000001(65, 0x41)
```

그 외의 문자는(즉 코드 포인트가 1바이트가 넘어가는 문자의 경우) 멀티바이트 사용을 하게 되는데 작동 원리는 다음과 같다.

```
UTF-8은 최대 4바이트까지 인코딩이 지원이 된다. 즉 MSB에 붙는 식별 비트는 다음과 같이 구분된다.

1바이트(아스키코드 선에서 해결 가능한)경우, '0xxx....'

2바이트의 경우, '110xxxx...'

3바이트의 경우, '1110xxxx...'

4바이트의 경우, '11110xxxx...'

여기서 중요한건 MSB쪽 8비트에 식별 비트를 붙이더라도 그 다음 바이트를 전부 사용하지는 않는다는 것이다.
2바이트 이상이 넘어가는 문자의 다른 바이트들은 문자를 표현하기 위해 최대 6비트 범위까지 사용이 가능하고, 이후 해당 바이트의 MSB쪽 비트에 '10'을 고정적으로 붙인다.

다음 예시를 보자.

유니코드상 ♣를 나타내는 이진수

00100110 01100011(9827, 0x2663)

ㄴ위에 설명처럼 인코딩 하면 3바이트로 인코딩 된다.

11100010 10011001 10100011

**즉 맨 앞 식별 비트와, 다른 바이트들에 붙는 '10'을 제외하고 기존 값을 오른쪽에서 순서대로 배치하는 것이다. 이런 배치 방식 때문에 3바이트로 인코딩 된다.**
```

## 문자를 사용한 수 표현

비트로 숫자를 표현할 수도 있고, 문자 역시 표현할 수도 있다. 그렇다면 문자를 사용한 수 표현. 즉 **2진 데이터를 문자로 표현하는 방법**에 대해선 어떤 것들이 있을까?

당시 사람들도 2진 데이터를 보내고 싶었으나, 이를 직접 보내는 것은 생각보다 어려웠다. 왜냐하면 아스키 코드의 상당 수는 제어 문자로 예약되어 있었고, 시스템마다 이러한 문자를 처리하는 방식이 달랐기 때문이다.

### QP(출력 가능하게 변경한)인코딩
  - 여기서 출력 가능이라는 표현은 아스키 코드에서 제어 문자가 아닌 문자라는 뜻이다.

QP인코딩은 전자우편과 같은 텍스트 기반 프로토콜에서 특수 문자를 안전하게 표현하기 위해 사용되는 인코딩 방식이다. 이 인코딩을 사용하면 = 다음에 바이트의 각 니블(한 바이트를 반으로 쪼갠 값, 16진수를 표현하기 최적화 된 단위이다.)을 표현하는 16진 숫자 2개를 추가해 8비트 값을 표현한다.

정확히는 문자 하나를 인코딩하는 경우, **문자 자체를 표현하기 위해 3개의 문자를 사용한다.** 구체적으로 다음과 같은 구조로 이루어진다:

- = 기호: QP 인코딩에서는 특수 문자를 인코딩할 때 앞에 = 기호를 붙인다. 이 = 기호는 해당 문자가 인코딩되었음을 나타내는 역할을 한다.
- 16진수 코드 두 자리: 문자에 해당하는 16진수 값 두 자리를 뒤에 붙인다.
- 예를 들어, 공백(ASCII 코드 32)을 QP 인코딩하면:

    - 공백의 ASCII 코드 값은 32이며, 이를 16진수로 변환하면 20이다.
    - 이를 QP 인코딩으로 표현하면 **=20**이 된다.
    - 이때 공백 문자 하나를 표현하기 위해 **3개의 아스키 문자**(**=, 2, 0**)를 사용하게 된다.

**여기서 = 라는 기호 역시 특별한 의미를 가지기 때문에 문자 '='를 표현하고 싶으면 아스키 코드상 = 에 위치하는 =3D를 사용해야한다.**

QP 인코딩은 몇 가지 규칙이 있는데 줄의 맨 끝에 탭과 공백문자가 온다면 이를 각각 =09와 =20으로 표현해야하며, 인코딩 된 데이터는 한 줄이 76자를 넘을 수가 없다. 어떤 줄의 맨 뒤가 =로 끝나면 가짜 줄바꿈을 뜻하며, 수신 쪽에서 QP로 인코딩된 데이터를 디코딩 할 때는 이 =를 제거하고 해석한다.
  - 가짜 줄바꿈은 송신하려는 문서의 내용은 아니지만 알아보기 쉽게 하기 위해 일부러 넣은 줄바꿈을 뜻한다. 위에 보듯이 한 줄에 76자로 제한을 두었기 때문에 그 이상되는 문장은 여러 줄로 나눠 써야 했고, 이때 나눠 쓴 각 줄의 맨 뒤에 있는 줄바꿈은 실제로는 원문의 줄바꿈이 아니므로 이를 표현하기 위해 =를 붙인다.


### 베이스64 인코딩

QP 인코딩의 성능은 문제가 없으나 1바이트를 표현하기 위해 3바이트를 사용하기 때문에 매우 비효율적이다. 지금도 효율성이 중요하지만, 먼 과거에는 통신 속도가 훨씬 느렸기에 더욱 중요한 요소였다. 이런 면에선 **베이스64** 인코딩이 더 효율적이다.

**베이스64 인코딩은 3바이트 데이터를 4문자로 표현한다.** 3바이트 데이터의 24비트를 6비트 덩어리로 나누고, 각 덩어리의 6비트값에 출력 가능한 문자를 할당해 표현한다.(자체적으로 할당한 값들이 존재한다. 자세한건 검색해보자..)

이 인코딩 방식은 모든 3바이트 조합을 4바이트 조합으로 변환할 수 있다. 문제는 **원본 데이터가 3바이트의 배수라는 보장은 그 어디에도 없다.** 이런 문제점을 **패딩 문자**를 도입해 해결했다. 예를 들어 원본 데이터가 2바이트 남는 경우엔 끝에 '='를 붙이고, 1바이트 남으면 끝에 ==를 붙인다.


#### Base64 인코딩의 예시: "Man" 문자열
아무리 저리 적어봤자 예시가 없으면 이해가 안될 것 같아서 예시를 가져왔다.(나부터 저것만 보고선 이해가 안됐다.)

Base64 인코딩을 설명하기 위해 간단한 예로 **Man**이라는 문자열을 Base64로 인코딩하는 과정을 살펴보자.

```
3바이트 데이터 준비:

"Man"이라는 문자열을 ASCII 코드로 변환하면 각각 다음과 같다:
M: 77 (2진수로 01001101)
a: 97 (2진수로 01100001)
n: 110 (2진수로 01101110)
3바이트 데이터를 2진수로 나열:

세 문자 M, a, n의 8비트씩을 나열하면 총 24비트의 데이터가 된다:

01001101 01100001 01101110
6비트 덩어리로 나누기:

이 24비트를 6비트씩 나누면, 다음과 같이 4개의 6비트 덩어리로 나뉜다:

010011 010110 000101 101110
6비트 덩어리를 10진수로 변환:

각 6비트 덩어리를 10진수로 변환하면:
010011: 19
010110: 22
000101: 5
101110: 46
Base64 문자 집합으로 매핑:

Base64는 총 64개의 문자로 이루어진 집합을 사용한다. 각 숫자를 Base64 문자 집합의 해당 위치에 매핑한다.
19: T
22: W
5: F
46: u
```

따라서, "Man"을 Base64로 인코딩하면 "TWFu"가 된다.

#### 패딩 문자 사용 예시
Base64 인코딩에서 원본 데이터가 3바이트의 배수가 아닌 경우, 패딩 문자를 사용해 데이터의 길이를 맞추어야 한다. 예를 들어 **Ma**라는 문자열을 Base64로 인코딩해 보자.

```
M: 77 (2진수로 01001101)
a: 97 (2진수로 01100001)
이 두 문자를 합치면 16비트가 된다:

01001101 01100001
이 16비트를 6비트 덩어리로 나누면:

010011 010110 0001
여기서 마지막 덩어리는 6비트가 되지 않기 때문에 2비트를 0으로 채워 6비트로 만든다

000100 (마지막 4비트 뒤에 2개의 0 추가)
6비트 덩어리를 10진수로 변환:

010011: 19
010110: 22
000100: 4
Base64 문자 집합으로 매핑:

19: T
22: W
4: E
패딩 문자 추가:

원래 데이터는 3바이트가 아니기 때문에, Base64 인코딩 결과는 항상 4문자가 되어야 한다.
따라서, 부족한 부분을 채우기 위해 **=**를 사용해 패딩한다.

```

최종 결과는 "TWE="가 된다.

### URL 인코딩

웹 페이지 URL에서도 위의 인코딩 방식들과 유사한 방식을 사용한다. URL에서 %26, %2F 같은 문자를 본 적이 있을 것이다. 이런 값들이 있는 이유는 **URL 이라는 문맥에서 몇몇 문자가 특별한 의미를 지니기 때문이다.**

이런 문자를 '문자 그대로' 사용하려면 어찌해야할까? 문자들은 그동안 봤듯이, 8비트 덩어리의 시퀀스로 표현된다. 이 말은 각 덩어리는 2개의 16진 문자로 표현 가능하다는 것이다.
**URL 인코딩은 퍼센트 인코딩이라고도 부르는데, % 뒤에 어떤 문자의 16진 표현을 덧붙이는 방식으로 문자를 인코딩 한다.**

예를 들어 슬래시 문자는 URL에서 특별한 의미를 지닌다. 이를 '문자 그대로' 사용하려면 슬래시 문자의 16진수인 2F를 사용하여 '%2F'라는 문자열로 대신한다.
**위 QP 인코딩 방식에서 유추할 수 있듯이, %에도 특별한 의미가 부여된 관계로 이 문자 역시 그대로 사용을 하려면 %25로 사용을 해야한다.**

## 색 표현

우리가 흔히 그래프의 좌표를 표현할 때 (1, 3) (1, 5, 2) 처럼 숫자 쌍을 사용한다. 컴퓨터에서 그래픽을 표현하는건 마치 모눈종이에 점을 찍어 그림을 만드는 과정이라고 보면 된다. 이때 모눈의 각 격자에 찍는 점을 Picture Element. 즉 그림 원소라고 부르고 이걸 줄인 말이 **Pixel**(픽셀) 이다.

이 픽셀에다가 각각 빨강, 초록, 파랑에 해당하는 광선을 섞어서 색을 만들어 내는 데, 이를 **RGB** 색 모델이라고 부른다.

원래 색은 컬러큐브라는 정육면체로 표현할 수 있는데, 컬러 큐브에서 각 축은 원색을 표현하며 값이 0과 1로 나타내진다. 값이 0이면 그에 해당하는 원소의 빛을 끈다는 의미이며, 1이면 최대 밝기로 켠다는 의미이다.
이를 다른 말로 하면 **아무 빛도 없으면 검은색이고 모든 빛을 최대로 켜면 흰색이라는 사실을 알 수 있다.**

여기에 각각 해당되는 색의 수치를 다르게 조합하여 여러 색을 만들 수 있는데 이런 식으로 빛을 혼합해 색을 표현하는 방식을 **가산** 색 시스템이라고 부른다. 말 그대로 각 빛을 섞을수록 더 밝은 색이 나오기 때문이다.
  - 손으로 그림을 그릴 때 익숙한 방법으로는 원색을 청록색, 자홍색, 노란색으로 가지는 감산식 시스템이 존재하는데 이는 가산 시스템과 다르게 각 색에 해당하지 않는 빛을 제거하면서 색을 만들어낸다.

두 시스템 중 가산 시스템이 더 많은 색을 만들어낼 수 있기 때문에 컴퓨터의 색 표현에는 가산 시스템이 주이다. 인간의 눈은 약 1천만 개의 색을 구분하는데, 현대 컴퓨터에선 이와 가장 비슷한 수인 24비트를 사용해 2의 제곱수에 해당하는 색을 표현할 수 있다.

실제 컴퓨터에선 24비트 단위로 계산을 수행하도록 설계되지 않았는데, 따라서 **색을 표현할 때는 가장 가까운 표준 크기인 32비트(4바이트)에 색을 넣어 처리한다.** 이쯤되면 문제점을 바로 발견했을 것이다. 색을 표현하는데 사용되지 않는 공간이 1바이트 생긴다는 점이다. 이는 지금 컴퓨터 기준으로 봐도 낭비되는 비트가 너무 많다. 이를 어찌 활용할 방법이 없을까? 해답은 색에 대해 논의하면서 다루지 않은 요소를 표현하기 위해 이 8비트를 사용하는 것이다. 바로 **투명도**이다.

### 투명도 표현

1984년 영화 제작사 루카스필름의 톰 더프와 토머스 포터는 투명도와 합성을 구현하는 새로운 방법을 발명했고, 이는 표준이 되었다. 이들은 각 픽셀에 **알파**라는 투명도 값을 추가했다.
  - 수학적으로 알파라는 값은 0 이상 1 이하인 값을 의미한다.
0은 해당 값이 완전히 투명하다는 의미이고 1은 완전히 불투명하다는 뜻이다. 이러한 여러 알파값들을 합성해 새로운 색을 만들어내는 방법을 정의하는 **합성 계산법**이었다. 이들은 부동소수점 수를 사용하지 않았기 때문에 1부터 255까지의 값을 알파의 값으로 활용했고, **이를 기존에 사용하지 않았던 1바이트의 공간을 활용했다.**

여기서 기존의 빨/녹/청색의 값을 그대로 저장하는게 아니라 **알파에 해당하는 8비트에 저장된 1부터 255의 값을 0과 1 사이의 값으로 치환하여 곱한 값을 저장한 것이다.** 예를 들어 색이 중간 정도 밝기의 빨간색이면 (200,0,0)이다. 이 색이 완전히 불투명한 경우엔 알파는 1이 되어 빨간색은 그대로 200의 값을 가진다. 여기서 절반의 투명도를 가졌다는 의미로 알파의 값이 약 0.5라면 **저장된 빨간색 값은 100이고 실제 알파의 저장된 수치는 255를 반으로 나눈 값을 반올림한 값인 127이 저장된다.**

이미지 합성은 색값을 알파로 곱하는 과정을 통해 이루어지며, 이때 이미 곱한 값을 저장함으로서 픽셀을 사용할 때 마다 알파를 곱하는 계산을 반복할 필요가 없다.

### 색 인코딩

웹 페이지는 사람이 읽을 수 있는 텍스트로 표현되기 때문에, 텍스트를 사용해 색을 표현할 방법 역시 필요했다. 상단의 URL 인코딩과 비슷한 방법으로 색을 인코딩 하는데, 웹에서는 색을 16진수 두개를 한 묶음으로 쳐 이어붙이는 식으로 표현한다. 간단한 예시를 보자.

```
인코딩을 할 때의 형식으로 #에다가 여섯 자리 16진 숫자를 추가하여 #rrggbb 처럼 표현한다.(여기서 rgb는 각각 빨/녹/청을 의미한다.)
각 색깔마다 0-255의 값을 가질 수 있기에 256을 16^2로 치환하여 #ffff00(노랑) #000000(검정) 등으로 표기할 수 있는 것이다.
```

## 정리

**개발 공부를 시작한 저 같은 초보자의 입장에서 컴퓨터의 기초 개념을 이해하는 것은 정말 중요하다고 생각합니다.** 쓸데없는 이론 공부처럼 느껴질 수 있지만, 실제로 프로그램을 작성할 때 하드웨어와 소프트웨어가 어떻게 소통하는지, 데이터가 어떻게 표현되고 처리되는지를 알고 모르고의 차이는 코드를 작성할 때 프로그래머가 생각하는 고민의 깊이를 다르게 만든다고 생각합니다. **더욱 중요한건 저처럼 개발을 넘어 컴퓨터 분야에 첫발을 내딘 사람이 다른 언어나 기술로 방향을 확장할 때 이러한 기본적인 개념은 학습을 하는데 있어서 두려움을 제거해주는 아주 중요한 정보가 될 수 있다고 확신합니다.**

아직은 갈 길이 멀었지만요...
